# -*- coding: utf-8 -*-
"""DatasetGenerator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AO1uHwXgGu5AWbsZR_wEc8zFE0-HuheR
"""

"""
smartgrid_passive_attack_dataset.py

Smart Grid Passive Attack Benchmarking Dataset Generator (IEEE-aligned HAN/NAN/WAN)
---------------------------------------------------------------------------------

Key properties
- Activity-gated labels and perturbations: attacks affect only time steps with tx_count > 0.
- Leak-safe splits: independent seeds + burn-in for AR/Markov-like processes.
- Topology-local attacks: connected subgraph (size 3â€“5) sampled from an IEEE-ish adjacency.
- Shared environment correlation: neighbors can share shadowing/interference trends.
- Heavy-tailed latency: bursty latency spikes driven by PER (tech-dependent).
- Tech-specific PER curves: avoids one-size-fits-all PER mapping.

Realism choice (Option A)
- 'shadow_db' and 'interf_db' are INTERNAL latent channel components.
  They are used to compute SNR/PER/latency, but are NOT exported as features.

Outputs (local disk only)
- Dataset/metadata.csv
- Dataset/metadata_ohe.csv
- Dataset/network_adjacency.csv
- Dataset/network_weights_W.csv
- Dataset/reason_vocab.json
- Dataset/attacks_windows_meta.csv
- Dataset/config.json
- Dataset/clients/client_nodeXX/{train,val,test}.csv
- Dataset/clients/client_nodeXX/train_stats.json

Run
    python smartgrid_passive_attack_dataset.py
"""

# ===========================
# 1) Imports
# ===========================
import json
import math
import os
import random
import shutil
from dataclasses import dataclass, asdict
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd


# ===========================
# 2) Configuration
# ===========================
@dataclass
class Config:
    # Output (LOCAL disk, not Google Drive)
    OUT_DIR: str = "Dataset"
    OVERWRITE: bool = True

    # Reproducibility
    SEED_BASE: int = 20251228

    # Time lengths per split (per node)
    T_TRAIN: int = 70000
    T_VAL: int = 15000
    T_TEST: int = 15000

    # Optional burn-in steps (not exported) to stabilize AR processes
    BURN_IN: int = 200

    # Sampling
    DT_SECONDS: float = 1.0

    # CSI amplitude observation noise
    CSI_NOISE_STD: float = 0.02

    # Latency model base (still uses rtx expectation)
    RTX_MS: float = 3.0
    MAX_RTX_EXPECT: float = 3.0
    LAT_SMOOTH_BETA: float = 0.90

    # Latency heavy-tail spike mechanism: shocks -> filtered bursts
    LAT_SPIKE_AR: float = 0.85

    # tx_count background process (role logic adds periodic/polling patterns)
    TX_BASE_RATE: float = 0.25
    TX_BURST_P: float = 0.010
    TX_BURST_LEN: Tuple[int, int] = (10, 60)
    TX_BURST_RATE_MULT: float = 6.0

    # Rare alarm bursts (role-dependent)
    ALARM_P_BASE: float = 0.0015
    ALARM_LEN: Tuple[int, int] = (5, 25)
    ALARM_RATE_MULT: float = 8.0

    # Attack coverage target: fraction over ACTIVE rows on eligible nodes
    TARGET_ATTACK_FRAC: float = 0.30

    # Attack windowing (core + lead/tail/hyst)
    WIN_CORE_MIN: int = 60
    WIN_CORE_MAX: int = 260
    LEAD: int = 8
    TAIL: int = 12
    HYST: int = 0

    # Attacked group size
    GROUP_MIN: int = 3
    GROUP_MAX: int = 5

    # Allow overlaps and uniqueness constraints
    ALLOW_OVERLAP: bool = True
    UNIQUE_KEY: bool = True

    # Passive propagation-level perturbations
    ATTACK_ALPHA_DROP_RANGE: Tuple[float, float] = (0.15, 0.30)   # coherence drop
    ATTACK_SIGMA_MULT_RANGE: Tuple[float, float] = (2.0, 3.5)     # scattering/innovation up
    ATTACK_SHADOW_LOSS_DB_RANGE: Tuple[float, float] = (1.2, 3.0) # extra path loss

    # Attack temporal profile
    RAMP_FRAC: float = 0.25

    # Activity gating
    MIN_ACTIVE_FRAC_CORE: float = 0.25
    MIN_ACTIVE_FRAC_LABELED: float = 0.15
    ACTIVITY_RESAMPLE_TRIES: int = 35

    # Effect spill (OFF)
    EFFECT_SPILL_P: float = 0.0
    EFFECT_SPILL_MAX: int = 0

    # Neighbor feature mixing
    SELF_MIX_ALPHA: float = 0.30

    # Rolling feature windows
    ENT_WIN: int = 25
    DRIFT_WIN: int = 10
    MOMENT_WIN: int = 25

    # Attack-eligible technologies
    ATTACK_ELIGIBLE_TECH: Tuple[str, ...] = ("ZigBee", "WiFi", "LTE", "LoRa", "PLC")

    # Shared environment correlation params
    ENV_SHADOW_W_GLOBAL: float = 0.35
    ENV_SHADOW_W_LAYER: float = 0.30
    ENV_INTERF_W_GLOBAL: float = 0.35
    ENV_INTERF_W_LAYER: float = 0.35

    # AR(1) for global/layer environment
    ENV_GLOBAL_ALPHA: float = 0.995
    ENV_LAYER_ALPHA: float = 0.990

    # Environment innovation scales (tech scaling applies in local terms)
    ENV_SHADOW_SIGMA_GLOBAL: float = 0.25
    ENV_SHADOW_SIGMA_LAYER: float = 0.35
    ENV_INTERF_SIGMA_GLOBAL: float = 0.22
    ENV_INTERF_SIGMA_LAYER: float = 0.28


cfg = Config()


# ===========================
# 3) Node metadata (edit freely)
# ===========================
NODES = [
    (0,  "SmartMeter",   "HAN", "ZigBee"),
    (1,  "SmartMeter",   "HAN", "ZigBee"),
    (2,  "SmartMeter",   "HAN", "WiFi"),
    (3,  "Gateway",      "HAN", "WiFi"),
    (4,  "DER",          "NAN", "LoRa"),
    (5,  "DER",          "NAN", "LoRa"),
    (6,  "FeederRelay",  "NAN", "PLC"),
    (7,  "Controller",   "NAN", "LTE"),
    (8,  "PMU",          "WAN", "Fiber"),
    (9,  "SCADA",        "WAN", "Fiber"),
    (10, "AMIHeadend",   "WAN", "LTE"),
    (11, "SubstationGW", "WAN", "PLC"),
]


# ===========================
# 4) Seeding + helpers
# ===========================
def seed_all(seed: int) -> None:
    random.seed(seed)
    np.random.seed(seed)


def ar1_process(T: int, alpha: float, sigma: float, x0: float = 0.0) -> np.ndarray:
    """Zero-mean AR(1) process used for correlated environment components."""
    out = np.zeros((T,), dtype=np.float32)
    out[0] = float(x0 + sigma * np.random.randn())
    for t in range(1, T):
        out[t] = float(alpha * out[t - 1] + sigma * np.random.randn())
    return out


# ===========================
# 5) Metadata + one-hot
# ===========================
def build_metadata(nodes: List[Tuple[int, str, str, str]]) -> Tuple[pd.DataFrame, pd.DataFrame]:
    df = pd.DataFrame(nodes, columns=["node", "role", "layer", "tech"]).sort_values("node").reset_index(drop=True)

    node_ids = df["node"].to_list()
    if node_ids != list(range(len(node_ids))):
        raise ValueError(
            f"Node IDs must be consecutive 0..n-1. Got: {node_ids}. "
            "Renumber nodes or add an ID->index mapping."
        )

    ohe = pd.get_dummies(df[["role", "layer", "tech"]], prefix=["role", "layer", "tech"])
    df_ohe = pd.concat([df[["node"]], ohe], axis=1)
    return df, df_ohe


# ===========================
# 6) IEEE-ish adjacency
# ===========================
def build_adjacency(df_meta: pd.DataFrame) -> np.ndarray:
    n = df_meta.shape[0]
    A = np.zeros((n, n), dtype=np.int32)

    def add_undirected(i: int, j: int) -> None:
        A[i, j] = 1
        A[j, i] = 1

    role_to_nodes: Dict[str, List[int]] = {}
    for _, r in df_meta.iterrows():
        role_to_nodes.setdefault(str(r["role"]), []).append(int(r["node"]))

    meters = role_to_nodes.get("SmartMeter", [])
    gateways = role_to_nodes.get("Gateway", [])
    ders = role_to_nodes.get("DER", [])
    relays = role_to_nodes.get("FeederRelay", [])
    controllers = role_to_nodes.get("Controller", [])
    pmus = role_to_nodes.get("PMU", [])
    scadas = role_to_nodes.get("SCADA", [])
    amis = role_to_nodes.get("AMIHeadend", [])
    subgws = role_to_nodes.get("SubstationGW", [])

    # HAN: meters <-> gateway(s)
    for m in meters:
        for g in gateways:
            add_undirected(m, g)

    # NAN: DER/relay <-> controller
    for d in ders + relays:
        for c in controllers:
            add_undirected(d, c)

    # Bridge: gateway <-> controller
    for g in gateways:
        for c in controllers:
            add_undirected(g, c)

    # WAN core: controller/substationGW <-> SCADA/AMI/PMU
    for c in controllers + subgws:
        for h in scadas + amis + pmus:
            add_undirected(c, h)

    # SubstationGW <-> controller
    for sg in subgws:
        for c in controllers:
            add_undirected(sg, c)

    # SCADA <-> AMI + PMU
    for s in scadas:
        for h in amis + pmus:
            add_undirected(s, h)

    return A


def row_stochastic_W(A: np.ndarray, self_mix_alpha: float = 0.3) -> np.ndarray:
    """Row-stochastic neighbor mixing matrix with self-mix."""
    n = A.shape[0]
    W = A.astype(np.float32)

    row_sum = W.sum(axis=1, keepdims=True)
    W = np.divide(W, np.maximum(row_sum, 1.0), out=np.zeros_like(W), where=(row_sum > 0))

    I = np.eye(n, dtype=np.float32)
    W = self_mix_alpha * I + (1.0 - self_mix_alpha) * W

    row_sum2 = W.sum(axis=1, keepdims=True)
    W = W / np.maximum(row_sum2, 1e-8)
    return W


# ===========================
# 7) Tech-specific dynamics
# ===========================
def per_tech_channel_params(tech: str) -> Tuple[float, float, float, float]:
    """
    Returns (alpha, sigma, shadow_sigma_db_local, interf_sigma_db_local) for local generation.
    alpha/sigma control AR(1) coherence of |H|, while the last two scale local shadow/interference.
    """
    if tech == "Fiber":
        return (0.985, 0.035, 0.30, 0.15)
    if tech == "PLC":
        return (0.86, 0.16, 1.60, 1.05)
    if tech == "LoRa":
        return (0.90, 0.13, 1.80, 0.75)
    if tech == "LTE":
        return (0.88, 0.14, 1.10, 1.10)
    if tech == "WiFi":
        return (0.92, 0.12, 0.95, 0.80)
    if tech == "ZigBee":
        return (0.93, 0.11, 1.10, 0.65)
    return (0.92, 0.12, 1.0, 0.6)


def per_tech_link_margin_db(tech: str) -> float:
    return {
        "ZigBee": 0.0,
        "WiFi": 1.0,
        "LTE": 2.0,
        "LoRa": -1.0,
        "PLC": 0.5,
        "Fiber": 6.0,
    }.get(tech, 0.0)


def per_tech_base_snr_db(tech: str) -> float:
    return {
        "ZigBee": 12.0,
        "WiFi": 16.0,
        "LTE": 18.0,
        "LoRa": 9.0,
        "PLC": 13.0,
        "Fiber": 28.0,
    }.get(tech, 14.0)


def per_tech_per_params(tech: str) -> Tuple[float, float]:
    """
    Tech-specific PER curve:
        PER = 1 / (1 + exp(k*(snr_db - snr50)))

    snr50: SNR where PER=0.5 (higher => needs better SNR)
    k    : steepness of the transition
    """
    return {
        "ZigBee": (9.5, 0.70),
        "WiFi":   (11.0, 0.75),
        "LTE":    (12.5, 0.70),
        "LoRa":   (7.5, 0.55),
        "PLC":    (10.5, 0.65),
        "Fiber":  (16.0, 0.85),
    }.get(tech, (10.0, 0.65))


def per_tech_latency_params(tech: str) -> Tuple[float, float, float, float]:
    """
    Returns (base_ms, jitter_std_ms, spike_p0, spike_scale_ms).

    spike_p0 and spike_scale_ms shape heavy-tailed burst behavior.
    PLC/LTE are configured to exhibit stronger tails than short-range HAN links,
    consistent with bursty interference and retransmission dynamics.
    """
    return {
        "Fiber":  (2.0, 0.25, 0.0008, 5.0),
        "WiFi":   (5.0, 0.80, 0.0030, 10.0),
        "ZigBee": (8.0, 1.00, 0.0035, 12.0),
        "LTE":    (7.0, 0.90, 0.0050, 14.0),
        "LoRa":   (35.0, 2.50, 0.0060, 35.0),
        "PLC":    (12.0, 1.30, 0.0075, 22.0),
    }.get(tech, (6.0, 0.8, 0.004, 12.0))


# ===========================
# 8) Traffic model (role-driven)
# ===========================
def _add_periodic_events(
    tx_1d: np.ndarray,
    period: int,
    jitter: int,
    amp_lo: float,
    amp_hi: float,
    dropout_p: float = 0.0,
) -> None:
    """Add periodic transmissions with jitter (in-place)."""
    T = tx_1d.shape[0]
    if period <= 1:
        return
    offset = int(np.random.randint(0, period))
    for base_t in range(offset, T, period):
        if dropout_p > 0 and np.random.rand() < dropout_p:
            continue
        t = int(base_t + np.random.randint(-jitter, jitter + 1))
        if 0 <= t < T:
            lam = float(np.random.uniform(amp_lo, amp_hi))
            tx_1d[t] += int(1 + np.random.poisson(lam))


def _add_alarm_bursts(
    tx_1d: np.ndarray,
    base_lam: float,
    p_alarm: float,
    rate_mult: float,
    L_rng: Tuple[int, int],
) -> None:
    """Rare alarm bursts (in-place)."""
    T = tx_1d.shape[0]
    t = 0
    while t < T:
        if np.random.rand() < p_alarm:
            Lb = int(np.random.randint(L_rng[0], L_rng[1] + 1))
            end = min(T, t + Lb)
            tx_1d[t:end] += np.random.poisson(base_lam * rate_mult, size=(end - t,))
            t = end
        else:
            t += 1


def gen_tx_count(n: int, T: int, meta: pd.DataFrame) -> np.ndarray:
    """
    Role-driven traffic:
    - SmartMeter: periodic reporting (+ jitter) + low background + rare alarms
    - SCADA: polling cycles + moderate background
    - PMU: near-continuous
    - Gateway/Controller: higher background + periodic forwarding/polling
    - DER/Relay: periodic + quiet periods
    - AMIHeadend/SubstationGW: periodic + moderate background
    """
    tx = np.zeros((n, T), dtype=np.int32)

    role_bg_lam = {
        "SmartMeter": 0.12,
        "Gateway": 0.45,
        "DER": 0.18,
        "FeederRelay": 0.22,
        "Controller": 0.55,
        "PMU": 2.20,
        "SCADA": 0.70,
        "AMIHeadend": 0.40,
        "SubstationGW": 0.35,
    }

    role_periodic = {
        "SmartMeter":   (60, 5, (1.0, 2.5), 0.05),
        "DER":          (30, 4, (0.6, 1.6), 0.10),
        "FeederRelay":  (25, 3, (0.8, 1.8), 0.10),
        "Gateway":      (15, 2, (0.8, 2.0), 0.02),
        "Controller":   (12, 2, (1.0, 2.2), 0.02),
        "SCADA":        (10, 1, (1.0, 2.5), 0.01),
        "AMIHeadend":   (20, 2, (0.8, 2.0), 0.03),
        "SubstationGW": (18, 2, (0.8, 2.0), 0.04),
        "PMU":          (2,  0, (0.5, 1.2), 0.00),
    }

    burst_p_role = {
        "SmartMeter": 0.006,
        "DER": 0.010,
        "FeederRelay": 0.012,
        "Gateway": 0.012,
        "Controller": 0.012,
        "PMU": 0.004,
        "SCADA": 0.010,
        "AMIHeadend": 0.010,
        "SubstationGW": 0.012,
    }

    alarm_p_role = {
        "SmartMeter": cfg.ALARM_P_BASE * 1.2,
        "DER": cfg.ALARM_P_BASE * 1.4,
        "FeederRelay": cfg.ALARM_P_BASE * 1.3,
        "Gateway": cfg.ALARM_P_BASE * 1.1,
        "Controller": cfg.ALARM_P_BASE * 1.0,
        "PMU": cfg.ALARM_P_BASE * 0.6,
        "SCADA": cfg.ALARM_P_BASE * 0.8,
        "AMIHeadend": cfg.ALARM_P_BASE * 0.9,
        "SubstationGW": cfg.ALARM_P_BASE * 1.0,
    }

    for i in range(n):
        role = str(meta.loc[i, "role"])
        bg_lam = float(role_bg_lam.get(role, cfg.TX_BASE_RATE))

        base = np.random.poisson(bg_lam, size=(T,)).astype(np.int32)

        # short bursty activity periods
        t = 0
        p_burst = float(burst_p_role.get(role, cfg.TX_BURST_P))
        while t < T:
            if np.random.rand() < p_burst:
                Lb = int(np.random.randint(cfg.TX_BURST_LEN[0], cfg.TX_BURST_LEN[1] + 1))
                end = min(T, t + Lb)
                base[t:end] += np.random.poisson(bg_lam * cfg.TX_BURST_RATE_MULT, size=(end - t,)).astype(np.int32)
                t = end
            else:
                t += 1

        # periodic patterns
        if role in role_periodic:
            period, jitter, (alo, ahi), dropout = role_periodic[role]
            _add_periodic_events(base, period=period, jitter=jitter, amp_lo=alo, amp_hi=ahi, dropout_p=dropout)

        # rare alarms
        _add_alarm_bursts(
            base,
            base_lam=max(0.15, bg_lam),
            p_alarm=float(alarm_p_role.get(role, cfg.ALARM_P_BASE)),
            rate_mult=cfg.ALARM_RATE_MULT,
            L_rng=cfg.ALARM_LEN,
        )

        # quiet periods
        if role in ("DER", "FeederRelay"):
            mask = (np.random.rand(T) < 0.15)
            base[mask] = 0

        # PMU near-continuous
        if role == "PMU":
            keep = (np.random.rand(T) < 0.85)
            base[keep] = np.maximum(base[keep], 1)

        tx[i, :] = base

    return tx


# ===========================
# 9) Channel amplitude |H|
# ===========================
def gen_channel_amp_per_node(meta: pd.DataFrame, T: int, mean: float = 1.0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """Generate |H| per node with tech-specific AR(1) dynamics."""
    n = meta.shape[0]
    H = np.zeros((n, T), dtype=np.float32)
    alpha_base = np.zeros((n,), dtype=np.float32)
    sigma_base = np.zeros((n,), dtype=np.float32)

    for i in range(n):
        tech = str(meta.loc[i, "tech"])
        alpha, sigma, _, _ = per_tech_channel_params(tech)
        alpha_base[i] = float(alpha)
        sigma_base[i] = float(sigma)

        H[i, 0] = float(mean + sigma * np.random.randn())
        for t in range(1, T):
            H[i, t] = float(alpha * H[i, t - 1] + (1 - alpha) * mean + sigma * np.random.randn())

        # PLC: occasional micro-fade bursts
        if tech == "PLC" and T > 50:
            n_bursts = int(np.random.randint(2, 6))
            for _ in range(n_bursts):
                s0 = int(np.random.randint(0, T - 10))
                L = int(np.random.randint(6, 25))
                s1 = min(T, s0 + L)
                H[i, s0:s1] = H[i, s0:s1] * (0.85 + 0.10 * np.random.rand())

        # LTE: slow variability
        if tech == "LTE":
            drift = 0.02 * np.cumsum(np.random.randn(T).astype(np.float32)) / max(1, np.sqrt(T))
            H[i, :] = H[i, :] * (1.0 + drift)

    H = np.abs(H)
    H = np.clip(H, 1e-3, None)
    return H.astype(np.float32), alpha_base, sigma_base


# ===========================
# 10) Correlated environment: shadowing + interference (INTERNAL only)
# ===========================
def gen_shadowing_db(meta: pd.DataFrame, T: int) -> np.ndarray:
    """
    shadow_db(i,t) = w_g*global(t) + w_l*layer(layer(i), t) + (1-w_g-w_l)*local(i,t)
    """
    n = meta.shape[0]
    out = np.zeros((n, T), dtype=np.float32)

    g = ar1_process(T, alpha=cfg.ENV_GLOBAL_ALPHA, sigma=cfg.ENV_SHADOW_SIGMA_GLOBAL)
    layers = meta["layer"].unique().tolist()
    layer_proc: Dict[str, np.ndarray] = {str(ly): ar1_process(T, cfg.ENV_LAYER_ALPHA, cfg.ENV_SHADOW_SIGMA_LAYER) for ly in layers}

    for i in range(n):
        tech = str(meta.loc[i, "tech"])
        layer = str(meta.loc[i, "layer"])
        _, _, sh_sigma_local, _ = per_tech_channel_params(tech)

        local = (sh_sigma_local * np.random.randn(T)).astype(np.float32)
        out[i, :] = (
            cfg.ENV_SHADOW_W_GLOBAL * g
            + cfg.ENV_SHADOW_W_LAYER * layer_proc[layer]
            + (1.0 - cfg.ENV_SHADOW_W_GLOBAL - cfg.ENV_SHADOW_W_LAYER) * local
        ).astype(np.float32)

    return out


def gen_interference_db(meta: pd.DataFrame, T: int) -> np.ndarray:
    """
    interf_db(i,t) = base + w_g*global(t) + w_l*layer(t) + (1-w_g-w_l)*local(i,t) + tech spikes
    """
    n = meta.shape[0]
    out = np.zeros((n, T), dtype=np.float32)

    g = ar1_process(T, alpha=cfg.ENV_GLOBAL_ALPHA, sigma=cfg.ENV_INTERF_SIGMA_GLOBAL)
    layers = meta["layer"].unique().tolist()
    layer_proc: Dict[str, np.ndarray] = {str(ly): ar1_process(T, cfg.ENV_LAYER_ALPHA, cfg.ENV_INTERF_SIGMA_LAYER) for ly in layers}

    for i in range(n):
        tech = str(meta.loc[i, "tech"])
        layer = str(meta.loc[i, "layer"])
        _, _, _, if_sigma_local = per_tech_channel_params(tech)

        eps = (if_sigma_local * np.random.randn(T)).astype(np.float32)
        ar = 0.80 if tech in ("PLC", "LTE") else 0.60
        local = np.zeros((T,), dtype=np.float32)
        local[0] = eps[0]
        for t in range(1, T):
            local[t] = ar * local[t - 1] + (1.0 - ar) * eps[t]

        base = float(1.2)

        spikes = np.zeros((T,), dtype=np.float32)
        if tech == "PLC":
            mask = (np.random.rand(T) < 0.012).astype(np.float32)
            spikes += mask * (1.4 + 2.2 * np.random.rand(T).astype(np.float32))
        elif tech == "LTE":
            mask = (np.random.rand(T) < 0.020).astype(np.float32)
            spikes += mask * (0.8 + 1.6 * np.random.rand(T).astype(np.float32))
        elif tech in ("WiFi", "ZigBee"):
            mask = (np.random.rand(T) < 0.010).astype(np.float32)
            spikes += mask * (0.4 + 0.8 * np.random.rand(T).astype(np.float32))

        out[i, :] = (
            base
            + cfg.ENV_INTERF_W_GLOBAL * g
            + cfg.ENV_INTERF_W_LAYER * layer_proc[layer]
            + (1.0 - cfg.ENV_INTERF_W_GLOBAL - cfg.ENV_INTERF_W_LAYER) * local
            + spikes
        ).astype(np.float32)

    return out


# ===========================
# 11) Channel chain: CSI -> SNR -> PER -> latency
# ===========================
def compute_per_from_snr_db(meta: pd.DataFrame, snr_db: np.ndarray) -> np.ndarray:
    n, _ = snr_db.shape
    snr50 = np.zeros((n, 1), dtype=np.float32)
    k = np.zeros((n, 1), dtype=np.float32)
    for i in range(n):
        tech = str(meta.loc[i, "tech"])
        s50, kk = per_tech_per_params(tech)
        snr50[i, 0] = float(s50)
        k[i, 0] = float(kk)

    per = 1.0 / (1.0 + np.exp(k * (snr_db - snr50)))
    return np.clip(per, 1e-5, 1.0 - 1e-5).astype(np.float32)


def compute_latency_ms(meta: pd.DataFrame, per: np.ndarray) -> np.ndarray:
    """
    latency_ms = base(tech) + RTX_MS * E[reTX(per)] + gaussian_jitter + heavy_tail_bursts(per)

    Heavy-tail bursts are driven by PER and filtered with an AR component to create clusters.
    """
    n, T = per.shape

    exp_rtx = per / np.maximum(1.0 - per, 1e-4)
    exp_rtx = np.clip(exp_rtx, 0.0, cfg.MAX_RTX_EXPECT).astype(np.float32)

    base_lat = np.zeros((n, 1), dtype=np.float32)
    jitter_std = np.zeros((n, 1), dtype=np.float32)
    spike_p0 = np.zeros((n, 1), dtype=np.float32)
    spike_scale = np.zeros((n, 1), dtype=np.float32)

    for i in range(n):
        tech = str(meta.loc[i, "tech"])
        b, js, p0, sc = per_tech_latency_params(tech)
        base_lat[i, 0] = float(b)
        jitter_std[i, 0] = float(js)
        spike_p0[i, 0] = float(p0)
        spike_scale[i, 0] = float(sc)

    jitter = (jitter_std * np.random.randn(n, T)).astype(np.float32)

    p1 = np.clip(
        0.03 + 0.06 * (spike_scale / np.maximum(spike_scale.mean(), 1e-3)),
        0.02,
        0.12,
    ).astype(np.float32)
    p = np.clip(spike_p0 + p1 * per, 0.0, 0.35).astype(np.float32)

    u = np.random.rand(n, T).astype(np.float32)
    shock = (u < p).astype(np.float32) * np.random.exponential(scale=1.0, size=(n, T)).astype(np.float32)
    shock = shock * spike_scale

    spikes = np.zeros((n, T), dtype=np.float32)
    spikes[:, 0] = shock[:, 0]
    ar = float(cfg.LAT_SPIKE_AR)
    for t in range(1, T):
        spikes[:, t] = ar * spikes[:, t - 1] + shock[:, t]

    lat = base_lat + cfg.RTX_MS * exp_rtx + jitter + spikes
    lat = np.clip(lat, 0.5, None).astype(np.float32)
    return lat


def smooth_ewma(x: np.ndarray, beta: float) -> np.ndarray:
    y = np.zeros_like(x, dtype=np.float32)
    y[:, 0] = x[:, 0]
    for t in range(1, x.shape[1]):
        y[:, t] = beta * y[:, t - 1] + (1.0 - beta) * x[:, t]
    return y


def run_channel_chain(
    meta: pd.DataFrame,
    H_amp: np.ndarray,
    shadow_db: np.ndarray,
    interf_db: np.ndarray,
) -> Dict[str, np.ndarray]:
    n, T = H_amp.shape

    # Measured amplitude observation noise (strictly non-negative on magnitude)
    csi_meas = H_amp + cfg.CSI_NOISE_STD * np.abs(np.random.randn(n, T)).astype(np.float32)
    csi_meas = np.clip(csi_meas, 1e-4, None)

    base_snr = np.zeros((n, 1), dtype=np.float32)
    link_margin = np.zeros((n, 1), dtype=np.float32)
    for i in range(n):
        tech = str(meta.loc[i, "tech"])
        base_snr[i, 0] = per_tech_base_snr_db(tech)
        link_margin[i, 0] = per_tech_link_margin_db(tech)

    snr_db = base_snr + 20.0 * np.log10(csi_meas) + link_margin + shadow_db - interf_db
    per = compute_per_from_snr_db(meta, snr_db)

    lat = compute_latency_ms(meta, per)
    lat_smooth = smooth_ewma(lat, cfg.LAT_SMOOTH_BETA)

    return {
        "H_amp": H_amp.astype(np.float32),
        "csi_amp": csi_meas.astype(np.float32),
        "snr_db": snr_db.astype(np.float32),
        "packet_error": per.astype(np.float32),
        "latency": lat.astype(np.float32),
        "latency_smooth": lat_smooth.astype(np.float32),
    }


# ===========================
# 12) Rolling engineered features
# ===========================
def rolling_entropy(x: np.ndarray, win: int, nbins: int = 12) -> np.ndarray:
    """Rolling Shannon entropy over a per-node histogram in a sliding window."""
    n, T = x.shape
    out = np.zeros((n, T), dtype=np.float32)

    xmin = np.quantile(x, 0.01, axis=1)
    xmax = np.quantile(x, 0.99, axis=1)

    for i in range(n):
        lo = float(xmin[i])
        hi = float(xmax[i])
        if hi <= lo:
            hi = lo + 1e-3
        edges = np.linspace(lo, hi, nbins + 1)

        for t in range(T):
            a = max(0, t - win + 1)
            seg = x[i, a: t + 1]
            hist, _ = np.histogram(seg, bins=edges)
            p = hist.astype(np.float32)
            p = p / np.maximum(p.sum(), 1.0)
            nz = p[p > 0]
            out[i, t] = float(-np.sum(nz * np.log(nz)))

    return out


def rolling_mean(x: np.ndarray, win: int) -> np.ndarray:
    n, T = x.shape
    out = np.zeros((n, T), dtype=np.float32)
    c = np.cumsum(x, axis=1, dtype=np.float64)
    for t in range(T):
        a = max(0, t - win + 1)
        denom = (t - a + 1)
        s = c[:, t] - (c[:, a - 1] if a > 0 else 0.0)
        out[:, t] = (s / denom).astype(np.float32)
    return out


def compute_time_since_last_tx(tx_count: np.ndarray) -> np.ndarray:
    n, T = tx_count.shape
    out = np.zeros((n, T), dtype=np.int32)
    for i in range(n):
        gap = 0
        for t in range(T):
            if tx_count[i, t] > 0:
                gap = 0
            else:
                gap += 1
            out[i, t] = gap
    return out


def add_rolling_moments_in_df(df: pd.DataFrame, win: int) -> pd.DataFrame:
    """Add per-node rolling skewness and kurtosis of csi_amp."""
    out = df.copy().sort_values(["node", "t"]).reset_index(drop=True)

    def _moments(g: pd.DataFrame) -> pd.DataFrame:
        s = g["csi_amp"].astype("float32")
        g["csi_skewness"] = s.rolling(win, min_periods=3).skew().fillna(0.0).astype("float32")
        g["csi_kurtosis"] = s.rolling(win, min_periods=4).kurt().fillna(0.0).astype("float32")
        return g

    out = out.groupby("node", group_keys=False).apply(_moments)
    out = out.sort_values(["t", "node"]).reset_index(drop=True)
    return out


# ===========================
# 13) Attack placement + perturbations
# ===========================
REASON_VOCAB = {"COHERENCE_DROP": 0}


def attack_eligible_nodes(meta: pd.DataFrame) -> List[int]:
    allowed = set(cfg.ATTACK_ELIGIBLE_TECH)
    return [int(r["node"]) for _, r in meta.iterrows() if str(r["tech"]) in allowed]


def pick_connected_group(A: np.ndarray, eligible: List[int], group_size: int, seed: Optional[int] = None) -> List[int]:
    """Grow a connected group from a chosen seed using adjacency A."""
    if group_size <= 1:
        return [int(seed) if seed is not None else int(np.random.choice(eligible))]

    eligible_set = set(int(x) for x in eligible)

    if seed is None:
        seed = int(np.random.choice(eligible))
    else:
        seed = int(seed)
        if seed not in eligible_set:
            seed = int(np.random.choice(eligible))

    group = [seed]
    frontier = [seed]

    while len(group) < group_size and frontier:
        u = int(frontier.pop(0))
        nbrs = np.where(A[u] > 0)[0].astype(int).tolist()
        nbrs = [v for v in nbrs if (v in eligible_set and v not in group)]
        random.shuffle(nbrs)
        for v in nbrs:
            group.append(v)
            frontier.append(v)
            if len(group) >= group_size:
                break

    if len(group) < group_size:
        rest = [v for v in eligible if v not in group]
        random.shuffle(rest)
        group.extend(rest[: (group_size - len(group))])

    return group


def place_attack_windows(
    tx_count: np.ndarray,
    meta: pd.DataFrame,
    A: np.ndarray,
    T: int,
    target_frac: float,
    split_name: str,
) -> Tuple[np.ndarray, List[Dict]]:
    """
    Create activity-gated attack labels y(node,t) on eligible nodes.

    Target coverage is enforced PER NODE over ACTIVE rows:
        target_rows_node = ceil(target_frac * active_rows_node)

    Labels are applied only where tx_count > 0 inside the labeled interval [s0, s1).
    """
    n = meta.shape[0]
    y = np.zeros((n, T), dtype=np.int8)

    eligible = attack_eligible_nodes(meta)
    if len(eligible) == 0:
        return y, []

    active = (tx_count > 0).astype(np.int16)
    active_pref = np.concatenate(
        [np.zeros((n, 1), dtype=np.int32), np.cumsum(active, axis=1, dtype=np.int32)],
        axis=1,
    )

    def active_frac(node: int, a: int, b: int) -> float:
        if b <= a:
            return 0.0
        a = max(0, a)
        b = min(T, b)
        cnt = int(active_pref[node, b] - active_pref[node, a])
        return cnt / float(b - a)

    active_rows_node = {nd: int(active[nd, :].sum()) for nd in eligible}
    target_rows_node = {nd: int(math.ceil(target_frac * max(1, active_rows_node[nd]))) for nd in eligible}
    attacked_rows_node = {nd: 0 for nd in eligible}

    def node_deficit(nd: int) -> int:
        return target_rows_node[nd] - attacked_rows_node[nd]

    def node_is_saturated(nd: int) -> bool:
        return attacked_rows_node[nd] >= int(1.15 * target_rows_node[nd])

    windows: List[Dict] = []
    used_keys = set()

    max_global_tries = 14000
    tries = 0

    while tries < max_global_tries and any(node_deficit(nd) > 0 for nd in eligible):
        tries += 1

        under = [nd for nd in eligible if node_deficit(nd) > 0]
        if not under:
            break

        weights = np.array([max(1, node_deficit(nd)) for nd in under], dtype=np.float64)
        weights = weights / weights.sum()
        anchor = int(np.random.choice(np.array(under, dtype=int), p=weights))

        L = int(np.random.randint(cfg.WIN_CORE_MIN, cfg.WIN_CORE_MAX + 1))
        L_ext = int(cfg.LEAD + L + cfg.TAIL + cfg.HYST)
        if L_ext >= T - 2:
            continue

        group_size = int(np.random.randint(cfg.GROUP_MIN, cfg.GROUP_MAX + 1))
        group_size = min(group_size, len(eligible))

        group = pick_connected_group(A, eligible, group_size, seed=anchor)

        if sum(1 for nd in group if node_is_saturated(int(nd))) >= max(2, len(group) // 2):
            continue

        gidx = np.array(group, dtype=int)

        ok_start: Optional[Tuple[int, int, int, int]] = None
        for _ in range(max(1, cfg.ACTIVITY_RESAMPLE_TRIES)):
            s0 = int(np.random.randint(0, T - L_ext))
            s1 = int(s0 + L_ext)
            t0 = int(s0 + cfg.LEAD)
            t1 = int(t0 + L)

            if cfg.UNIQUE_KEY:
                # Include window end to avoid rejecting valid different-length windows
                key = (split_name, t0, t1, tuple(sorted(group)))
                if key in used_keys:
                    continue

            if not cfg.ALLOW_OVERLAP and np.any(y[gidx, s0:s1] == 1):
                continue

            ok = True
            for nd in group:
                nd = int(nd)
                if cfg.MIN_ACTIVE_FRAC_CORE > 0 and active_frac(nd, t0, t1) < cfg.MIN_ACTIVE_FRAC_CORE:
                    ok = False
                    break
                if cfg.MIN_ACTIVE_FRAC_LABELED > 0 and active_frac(nd, s0, s1) < cfg.MIN_ACTIVE_FRAC_LABELED:
                    ok = False
                    break
                if node_is_saturated(nd):
                    ok = False
                    break

            if ok:
                ok_start = (s0, s1, t0, t1)
                if cfg.UNIQUE_KEY:
                    used_keys.add(key)
                break

        if ok_start is None:
            continue

        s0, s1, t0, t1 = ok_start

        before_vec = y[gidx, :].sum(axis=1).astype(np.int32)

        mask_active = (tx_count[gidx, s0:s1] > 0)
        y[gidx, s0:s1] = np.maximum(y[gidx, s0:s1], mask_active.astype(np.int8))

        after_vec = y[gidx, :].sum(axis=1).astype(np.int32)
        delta_vec = (after_vec - before_vec)

        for k, nd in enumerate(group):
            nd = int(nd)
            if nd in attacked_rows_node:
                attacked_rows_node[nd] += int(delta_vec[k])

        windows.append(
            {
                "split": split_name,
                "s0": int(s0),
                "s1": int(s1),
                "t0": int(t0),
                "t1": int(t1),
                "nodes": group,
                "reason": "COHERENCE_DROP",
                "alpha_drop": float(np.random.uniform(*cfg.ATTACK_ALPHA_DROP_RANGE)),
                "sigma_mult": float(np.random.uniform(*cfg.ATTACK_SIGMA_MULT_RANGE)),
                "shadow_loss_db": float(np.random.uniform(*cfg.ATTACK_SHADOW_LOSS_DB_RANGE)),
            }
        )

    return y, windows


def apply_attack_perturbations(
    meta: pd.DataFrame,
    streams: Dict[str, np.ndarray],
    windows: List[Dict],
    tx_count_full: np.ndarray,
    alpha_base: np.ndarray,
    sigma_base: np.ndarray,
) -> None:
    """
    Apply passive propagation perturbations on ACTIVE rows only:
    - shadow loss increases (shadow_db decreases in this sign convention)
    - coherence drop: lower AR alpha and higher innovation sigma on |H|

    After perturbations, recompute csi_amp/snr_db/per/latency from the channel chain.
    """
    H = streams["H_amp"].copy()
    shadow = streams["shadow_db"].copy()
    interf = streams["interf_db"].copy()  # unchanged by attack in this variant

    for w in windows:
        s0, s1 = int(w["s0"]), int(w["s1"])
        nodes = list(w["nodes"])

        Llab = max(1, s1 - s0)
        ramp_len = max(1, int(round(cfg.RAMP_FRAC * Llab)))

        ramp = np.ones((Llab,), dtype=np.float32)
        if ramp_len > 1:
            ramp[:ramp_len] = np.linspace(0.0, 1.0, ramp_len).astype(np.float32)

        alpha_drop = float(w["alpha_drop"])
        sigma_mult = float(w["sigma_mult"])
        sh_loss = float(w["shadow_loss_db"])

        for nd in nodes:
            act = (tx_count_full[nd, s0:s1] > 0).astype(np.float32)
            if act.sum() <= 0:
                continue

            shadow[nd, s0:s1] = shadow[nd, s0:s1] - (sh_loss * ramp * act)

            alpha0 = float(alpha_base[nd])
            sigma0 = float(sigma_base[nd])

            alpha_a = float(np.clip(alpha0 * (1.0 - alpha_drop), 0.50, 0.995))
            sigma_a = float(np.clip(sigma0 * sigma_mult, 0.01, 1.0))

            mean = 1.0
            prev = float(H[nd, s0 - 1] if s0 > 0 else H[nd, 0])

            for k, t in enumerate(range(s0, s1)):
                if act[k] <= 0.0:
                    prev = float(H[nd, t])
                    continue
                eps = float(np.random.randn())
                new_val = alpha_a * prev + (1.0 - alpha_a) * mean + sigma_a * eps
                H[nd, t] = float((1.0 - ramp[k]) * H[nd, t] + ramp[k] * new_val)
                prev = float(H[nd, t])

    H = np.abs(H)
    H = np.clip(H, 1e-3, None)

    out = run_channel_chain(meta, H_amp=H, shadow_db=shadow, interf_db=interf)

    streams["H_amp"] = out["H_amp"]
    streams["csi_amp"] = out["csi_amp"]
    streams["snr_db"] = out["snr_db"]
    streams["packet_error"] = out["packet_error"]
    streams["latency"] = out["latency"]
    streams["latency_smooth"] = out["latency_smooth"]

    streams["shadow_db"] = shadow
    streams["interf_db"] = interf


# ===========================
# 14) Z-score per node (fit on train only)
# ===========================
def fit_zscore_per_node(train_df: pd.DataFrame, cols: List[str]) -> Dict[int, Dict[str, Tuple[float, float]]]:
    stats: Dict[int, Dict[str, Tuple[float, float]]] = {}
    for node, g in train_df.groupby("node"):
        node = int(node)
        stats[node] = {}
        for c in cols:
            mu = float(g[c].mean())
            sd = float(g[c].std(ddof=0))
            if not np.isfinite(sd) or sd < 1e-6:
                sd = 1.0
            stats[node][c] = (mu, sd)
    return stats


def apply_zscore(df: pd.DataFrame, stats: Dict[int, Dict[str, Tuple[float, float]]], cols: List[str], prefix: str = "z_") -> pd.DataFrame:
    out = df.copy()
    for c in cols:
        z = np.zeros((len(out),), dtype=np.float32)
        for node, idx in out.groupby("node").groups.items():
            node = int(node)
            mu, sd = stats[node][c]
            arr = out.loc[idx, c].to_numpy(dtype=np.float32)
            z[idx] = (arr - mu) / sd
        out[prefix + c] = z
    return out


# ===========================
# 15) Neighbor features
# ===========================
def compute_neighbor_features(df_all: pd.DataFrame, W: np.ndarray, cols: List[str]) -> pd.DataFrame:
    out = df_all.copy().sort_values(["t", "node"]).reset_index(drop=True)
    n = W.shape[0]
    T = out["t"].nunique()

    if len(out) != n * T:
        raise ValueError("compute_neighbor_features expects complete grid: one row per (node,t).")

    for c in cols:
        X = out[c].to_numpy(dtype=np.float32).reshape(T, n).T  # (n,T)
        Xn = (W @ X)
        avg = Xn.T.reshape(-1)
        out[f"avg_neighbor_{c}"] = avg
        out[f"rho_like_{c}"] = np.abs(out[c].to_numpy(dtype=np.float32) - avg)

    return out


# ===========================
# 16) Build per-split DataFrames (Option A: NO shadow_db/interf_db columns)
# ===========================
def build_split_df(
    meta: pd.DataFrame,
    streams_export: Dict[str, np.ndarray],
    tx_count: np.ndarray,
    attack_label: np.ndarray,
    split_name: str,
) -> pd.DataFrame:
    n, T = tx_count.shape
    t_idx = np.arange(T, dtype=np.int32)
    rows = []

    # Exported features only: csi_amp, snr_db, packet_error, latency, latency_smooth
    for node in range(n):
        df = pd.DataFrame({
            "t": t_idx,
            "node": node,
            "split": split_name,
            "attack_label": attack_label[node, :].astype(np.int8),
            "tx_count": tx_count[node, :].astype(np.int32),
            "csi_amp": streams_export["csi_amp"][node, :].astype(np.float32),
            "snr_db": streams_export["snr_db"][node, :].astype(np.float32),
            "packet_error": streams_export["packet_error"][node, :].astype(np.float32),
            "latency": streams_export["latency"][node, :].astype(np.float32),
            "latency_smooth": streams_export["latency_smooth"][node, :].astype(np.float32),
        })
        rows.append(df)

    out = pd.concat(rows, ignore_index=True)

    # Engineered features (matrix-first)
    out_sorted = out.sort_values(["node", "t"]).reset_index(drop=True)
    csi_mat = out_sorted["csi_amp"].to_numpy(dtype=np.float32).reshape(n, T)

    ent = rolling_entropy(csi_mat, win=cfg.ENT_WIN)
    csi_smooth = rolling_mean(csi_mat, win=cfg.DRIFT_WIN)
    drift = (csi_mat - csi_smooth).astype(np.float32)

    out_sorted["csi_entropy"] = ent.T.reshape(-1)
    out_sorted["csi_drift"] = drift.T.reshape(-1)

    tx_mat = out_sorted["tx_count"].to_numpy(dtype=np.int32).reshape(n, T)
    tsl = compute_time_since_last_tx(tx_mat)
    out_sorted["time_since_last_tx"] = tsl.T.reshape(-1)

    # rolling skew/kurt
    out_sorted = add_rolling_moments_in_df(out_sorted, win=cfg.MOMENT_WIN)

    out_sorted = out_sorted.sort_values(["t", "node"]).reset_index(drop=True)
    return out_sorted


# ===========================
# 17) Generate one split
# ===========================
def generate_split(
    meta: pd.DataFrame,
    A: np.ndarray,
    split_name: str,
    T_export: int,
    seed: int,
) -> Tuple[pd.DataFrame, List[Dict]]:
    seed_all(seed)

    n = meta.shape[0]
    T = int(cfg.BURN_IN + T_export)

    tx = gen_tx_count(n, T, meta)

    H, alpha_base, sigma_base = gen_channel_amp_per_node(meta, T, mean=1.0)

    # Internal correlated environment (NOT exported as features)
    shadow = gen_shadowing_db(meta, T)
    interf = gen_interference_db(meta, T)

    ch = run_channel_chain(meta, H_amp=H, shadow_db=shadow, interf_db=interf)

    streams: Dict[str, np.ndarray] = {
        **ch,
        "shadow_db": shadow.astype(np.float32),
        "interf_db": interf.astype(np.float32),
    }

    tx_export = tx[:, cfg.BURN_IN:]

    y_export, windows = place_attack_windows(
        tx_count=tx_export,
        meta=meta,
        A=A,
        T=T_export,
        target_frac=cfg.TARGET_ATTACK_FRAC,
        split_name=split_name,
    )

    # Shift windows to global time (includes burn-in)
    windows_global: List[Dict] = []
    for w in windows:
        wg = dict(w)
        wg["s0"] = int(w["s0"] + cfg.BURN_IN)
        wg["s1"] = int(w["s1"] + cfg.BURN_IN)
        wg["t0"] = int(w["t0"] + cfg.BURN_IN)
        wg["t1"] = int(w["t1"] + cfg.BURN_IN)
        windows_global.append(wg)

    apply_attack_perturbations(
        meta=meta,
        streams=streams,
        windows=windows_global,
        tx_count_full=tx,
        alpha_base=alpha_base,
        sigma_base=sigma_base,
    )

    # Export only observable/derived streams (Option A)
    streams_export = {
        "csi_amp": streams["csi_amp"][:, cfg.BURN_IN:],
        "snr_db": streams["snr_db"][:, cfg.BURN_IN:],
        "packet_error": streams["packet_error"][:, cfg.BURN_IN:],
        "latency": streams["latency"][:, cfg.BURN_IN:],
        "latency_smooth": streams["latency_smooth"][:, cfg.BURN_IN:],
    }

    df = build_split_df(meta, streams_export, tx_export, y_export, split_name)

    for w in windows:
        w["n_nodes"] = int(len(w["nodes"]))
        w["L_core"] = int(w["t1"] - w["t0"])
        w["L_labeled"] = int(w["s1"] - w["s0"])

    return df, windows


# ===========================
# 18) Save per-client CSVs
# ===========================
def ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def write_clients(df_split: pd.DataFrame, out_dir: str, split_name: str) -> None:
    base = os.path.join(out_dir, "clients")
    ensure_dir(base)

    for node, g in df_split.groupby("node"):
        node = int(node)
        d = os.path.join(base, f"client_node{node:02d}")
        ensure_dir(d)
        g_sorted = g.sort_values("t").reset_index(drop=True)
        g_sorted.to_csv(os.path.join(d, f"{split_name}.csv"), index=False)


# ===========================
# 19) Orchestrate end-to-end
# ===========================
def main() -> None:
    out_dir = os.path.abspath(cfg.OUT_DIR)

    if cfg.OVERWRITE and os.path.exists(out_dir):
        shutil.rmtree(out_dir)
    ensure_dir(out_dir)

    df_meta, df_ohe = build_metadata(NODES)
    df_meta.to_csv(os.path.join(out_dir, "metadata.csv"), index=False)
    df_ohe.to_csv(os.path.join(out_dir, "metadata_ohe.csv"), index=False)

    A = build_adjacency(df_meta)
    W = row_stochastic_W(A, self_mix_alpha=cfg.SELF_MIX_ALPHA)

    pd.DataFrame(A, index=df_meta["node"], columns=df_meta["node"]).to_csv(
        os.path.join(out_dir, "network_adjacency.csv")
    )
    pd.DataFrame(W, index=df_meta["node"], columns=df_meta["node"]).to_csv(
        os.path.join(out_dir, "network_weights_W.csv")
    )

    with open(os.path.join(out_dir, "reason_vocab.json"), "w") as f:
        json.dump(REASON_VOCAB, f, indent=2)

    splits = {"train": cfg.T_TRAIN, "val": cfg.T_VAL, "test": cfg.T_TEST}

    df_splits: Dict[str, pd.DataFrame] = {}
    all_windows: List[Dict] = []

    for split_name, T in splits.items():
        seed = cfg.SEED_BASE + {"train": 11, "val": 23, "test": 37}[split_name]
        df, windows = generate_split(df_meta, A, split_name, T_export=T, seed=seed)
        df_splits[split_name] = df
        all_windows.extend(windows)

        eligible = set(attack_eligible_nodes(df_meta))
        mask_elig = df["node"].isin(list(eligible))

        cov_overall = float(df.loc[mask_elig, "attack_label"].mean())

        active_elig_rows = int(((df["tx_count"] > 0) & mask_elig).sum())
        attack_active_rows = int(((df["attack_label"] == 1) & (df["tx_count"] > 0) & mask_elig).sum())
        cov_active = float(attack_active_rows / max(1, active_elig_rows))

        silent_attack_rows = int(((df["attack_label"] == 1) & (df["tx_count"] == 0) & mask_elig).sum())
        attack_rows = int(((df["attack_label"] == 1) & mask_elig).sum())
        silent_attack = float(silent_attack_rows / max(1, attack_rows))

        print(f"[{split_name.upper()}] coverage (overall eligible rows): {cov_overall:.4f}")
        print(f"[{split_name.upper()}] coverage (ACTIVE eligible rows):  {cov_active:.4f} (target={cfg.TARGET_ATTACK_FRAC})")
        print(f"[{split_name.upper()}] silent fraction among y=1 (eligible): {silent_attack:.4f}")

    # Neighbor features
    neighbor_cols = ["snr_db", "latency_smooth", "csi_amp", "packet_error"]
    for split_name, df in df_splits.items():
        df_splits[split_name] = compute_neighbor_features(df, W, cols=neighbor_cols)

    # Z-score: fit on train only
    # Option A: no shadow_db/interf_db columns exist, so nothing to remove here.
    z_cols = [
        "snr_db",
        "packet_error",
        "latency",
        "latency_smooth",
        "csi_amp",
        "csi_entropy",
        "csi_drift",
        "csi_skewness",
        "csi_kurtosis",
        "tx_count",
        "time_since_last_tx",
        "avg_neighbor_snr_db",
        "avg_neighbor_latency_smooth",
        "avg_neighbor_csi_amp",
        "avg_neighbor_packet_error",
        "rho_like_snr_db",
        "rho_like_latency_smooth",
        "rho_like_csi_amp",
        "rho_like_packet_error",
    ]

    stats = fit_zscore_per_node(df_splits["train"], z_cols)

    # Write per-client train_stats.json (only that node's stats)
    base_clients = os.path.join(out_dir, "clients")
    ensure_dir(base_clients)
    for node in range(len(NODES)):
        d = os.path.join(base_clients, f"client_node{node:02d}")
        ensure_dir(d)
        with open(os.path.join(d, "train_stats.json"), "w") as f:
            json.dump(
                {
                    "node": int(node),
                    "stats": {c: [float(mu), float(sd)] for c, (mu, sd) in stats[int(node)].items()},
                },
                f,
                indent=2,
            )

    for split_name in ("train", "val", "test"):
        df_splits[split_name] = apply_zscore(df_splits[split_name], stats, z_cols, prefix="z_")

    # Write clients
    for split_name in ("train", "val", "test"):
        write_clients(df_splits[split_name], out_dir, split_name)

    # Write windows meta
    win_rows = []
    for w in all_windows:
        win_rows.append(
            {
                "split": w["split"],
                "s0": int(w["s0"]),
                "s1": int(w["s1"]),
                "t0": int(w["t0"]),
                "t1": int(w["t1"]),
                "nodes": ",".join(map(str, w["nodes"])),
                "n_nodes": int(w.get("n_nodes", len(w["nodes"]))),
                "L_core": int(w.get("L_core", w["t1"] - w["t0"])),
                "L_labeled": int(w.get("L_labeled", w["s1"] - w["s0"])),
                "reason": w.get("reason", "COHERENCE_DROP"),
                "alpha_drop": float(w.get("alpha_drop", 0.0)),
                "sigma_mult": float(w.get("sigma_mult", 1.0)),
                "shadow_loss_db": float(w.get("shadow_loss_db", 0.0)),
            }
        )

    pd.DataFrame(win_rows).to_csv(os.path.join(out_dir, "attacks_windows_meta.csv"), index=False)

    with open(os.path.join(out_dir, "config.json"), "w") as f:
        json.dump(asdict(cfg), f, indent=2)

    print("Done. Dataset written to:", out_dir)


if __name__ == "__main__":
    main()